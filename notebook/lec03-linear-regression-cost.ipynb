{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 03. How to minimize cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis and Cost\n",
    "\n",
    "$$H(x) = Wx + b$$\n",
    "\n",
    "\n",
    "$$cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} ((H(x_i)-y_i)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SImplified hypothesis\n",
    "\n",
    "$$H(x) = Wx$$\n",
    "\n",
    "\n",
    "$$cost(W) = \\frac{1}{m} \\sum^m_{i=1} (Wx_i-y_i)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What $cost(W)$ looks like ?\n",
    "\n",
    "| x | y |\n",
    "|:--:--:|\n",
    "| 1 | 1 |\n",
    "| 2 | 2 |\n",
    "| 3 | 3 |\n",
    "\n",
    "$$cost(W) = \\frac{1}{m} \\sum^m_{i=1} (Wx_i-y_i)^2 $$\n",
    "\n",
    "- if $W = 1$, $cost(W) = 0$\n",
    "$$\\frac{1}{3}(1 \\times 1 - 1)^2 + (1 \\times 2 - 2)^2 + (1 \\times 3 - 3)^2 = 0$$\n",
    "\n",
    "- if $W = 0$, $cost(W) = 4.67$\n",
    "$$\\frac{1}{3}(1 \\times 1 - 1)^2 + (0 \\times 2 - 2)^2 + (0 \\times 3 - 3)^2 = 4.67$$\n",
    "\n",
    "- if $W = 2$, $cost(W) = 4.67$\n",
    "$$\\frac{1}{3}(2 \\times 1 - 1)^2 + (2 \\times 2 - 2)^2 + (2 \\times 3 - 3)^2 = 4.67$$\n",
    "\n",
    "$$\\vdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent algorithm\n",
    "- Minimize cost function\n",
    "- Gradient descent is used many minimization problems\n",
    "- For a given cost function, $cost(W, b)$, it will find $W, b$ to minimize cost\n",
    "- It can be applied to more general function : $cost(w_1, w_2, \\cdots)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works ?\n",
    "- How would you find the lowest point ?\n",
    "\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-1ec77cdbb354c3b9d439fbe436dc5d4f\", width=400>\n",
    "\n",
    "- Start with initial guesses\n",
    " - Start at 0, 0 ( or any other value )\n",
    " - Keeping changing $W$ and $b$ a little bit to try and reduce $cost(W, b)$\n",
    "- Each time you change the parameters, you select the gradient which reduces $cost(W, b)$ the most possible\n",
    "- Repeat\n",
    "- Do so until you converge to a local minimum\n",
    "- Have an interesting property\n",
    " - Where you start can determine which minimum you end up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal definition\n",
    "- 미분시 계산 편리하게 하기 위해 분모를 $2m$ 으로 변경 ( 이렇게 해도 min point 찾는데는 지장 없음 )\n",
    "\n",
    "$$cost(W) = \\frac{1}{2m} \\sum^m_{i=1} (Wx_i-y_i)^2 $$\n",
    "\n",
    "- 현재 $W$ 값에 $\\alpha \\frac{\\partial}{\\partial W} cost(W)$ 을 뺀 것이 다음 $W$ 값\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W :=& W - \\alpha \\frac{\\partial}{\\partial W} cost(W) \\\\\n",
    "& \\alpha : learning ~ rate \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $W$ 로 $cost(W)$ 을 미분 **( Graident descent algorithm )**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W :=& W - \\alpha \\frac{\\partial}{\\partial W} \\frac{1}{2m} \\sum^m_{i=1} (Wx_i-y_i)^2 \\\\\n",
    "W :=& W - \\alpha \\frac{1}{m} \\sum^m_{i=1} (Wx_i-y_i) x_i \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex function\n",
    "- $cost(W, b)$ 의 모양이 Convex function 이 되어야 어느 지점에서 알고리즘을 시작하더라도 min point 가 일정하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
