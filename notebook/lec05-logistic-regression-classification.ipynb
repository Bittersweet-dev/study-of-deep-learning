{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5. Logistic ( regression ) classification\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "- Hypothesis:\n",
    "\n",
    "$$H(X) = WX$$\n",
    "\n",
    "- Cost : \n",
    "\n",
    "$$cost(W) = \\frac{1}{m} \\sum(WX - y)^2$$\n",
    "\n",
    "- Gradient descent :\n",
    " - $\\alpha$ : learning rate\n",
    " \n",
    "$$W := W - \\alpha \\frac{\\partial}{\\partial W} cost(W)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "- Spam Email Detection: Spam or Ham\n",
    "- Facebook feed: show or hide\n",
    "- Credit Card Fraudulent Transaction detection: legitimate or fraud\n",
    "- Radiology: Malignant tumor or Benign tumor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakness of linear regression\n",
    "- 만약 data 에서 아주 극단의 feature x 값이 있는 경우, 즉 outlier 가 있는 경우 line 이 기울어지게 됨\n",
    " - 편향된 line 에 의해 classification 오류가 발생할 수 있음\n",
    "- $H(x) = Wx + b$ 로는 $y = 0 ~ or ~ 1$ 을 표현하는데 한계가 있음\n",
    "\n",
    "<img src=\"http://www.ats.ucla.edu/stat/stata/webbooks/logistic/chapter1/stata9-1.gif\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Hypothesis\n",
    "- Logistic function\n",
    "- Sigmoid function\n",
    " - sigmoid is curved in two directions, like the letter \"S\", or the Greek $\\varsigma$ (sigma)\n",
    "$$g(z) = \\frac{1}{(1 + e^{-z})}$$\n",
    "\n",
    "### Hypothesis\n",
    "$$\n",
    "\\begin{align}\n",
    "H(x) &= g(z) \\\\\n",
    "( z &= WX )\\\\\n",
    "H(X) &= \\frac{1}{1+e^{-W^T X}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.saedsayad.com/images/LogReg_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost fucntion\n",
    "- cost\n",
    "$$cost(W, b) = \\frac{1}{m} \\sum (H(x_i) - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $H(x) = Wx + b$\n",
    " - $H(x)$ 는 직선\n",
    " - 직선을 제곱하면 곡선이 된다.\n",
    "![](https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcTQDO-s0PxY8dprDOOp2wjTAKTp_gs1HhESbyC05Z_tuEEK80wF)\n",
    "\n",
    "- $H(X) = \\frac{1}{1+e^{-W^T X}}$\n",
    " - $H(X)$ 는 $S$ 자 모양, $(0 \\le H(X) \\le 1)$\n",
    " - 제곱하면 구불구불한 곡선이 된다.\n",
    " - 이 경우 global minimum 을 찾지 못하고, local minimum 에 빠지기 쉽다.\n",
    " \n",
    "![](https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcQSo6e9_wzEYMfavVgKmqqXYzaJE7nVmRS13FuLmOISDgue3b_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New cost function for logistic\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "cost(W) &= \\frac{1}{m} \\sum c(H(x), y) \\\\\n",
    "c(H(x), y) &= \\begin{cases} -log(H(x)) \\quad y = 1 \\\\ -log(1 - H(x)) \\quad y = 0 \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "<br />\n",
    "\n",
    "- $H(X) = \\frac{1}{1+e^{-W^T X}}$ -> $log (H(X))$\n",
    " - exponential term 이 있기 때문에 구부러진 점을 상쇄하기 위해 $log$ 를 취해준다.\n",
    "\n",
    "\n",
    "- $z = H(X), \\quad c(H, y) = g(z)$\n",
    " - $g(z) = -log(z), \\quad when ~ y = 1$\n",
    "   - $z = 1$ 이 되면 $g(z)$ 는 $0$에 가까워짐 **( 예측이 맞음: $H(X) = 1$ -> $c(H, y) = 0$ )**\n",
    "   - $z = 0$ 에 가까워지면 $g(z)$ 는 $+\\infty$에 가까워짐 **( 예측이 틀림: $H(X) = 0$ -> $c(H, y) = \\infty$ )**\n",
    "   \n",
    "  - $g(z) = -log(1 - z), \\quad when ~ y = 0$\n",
    "   - $z = 0$ 이 되면 $g(z)$ 는 $0$에 가까워짐 **( 예측이 맞음: $H(X) = 0$ -> $c(H, y) = 0$ )**\n",
    "   - $z = 1$ 에 가까워지면 $g(z)$ 는 $+\\infty$에 가까워짐 **( 예측이 틀림: $H(X) = 1$ -> $c(H, y) = \\infty$ )**\n",
    "\n",
    "![](http://adit.io/imgs/logistic/log_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "- if 문을 없애고 한 줄 짜리 수식으로 바꿔줌\n",
    " - $y = 1, \\quad c = -log(H(x))$\n",
    " - $y = 0, \\quad c = -log(1 - H(x))$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "cost(W) &= \\frac{1}{m} \\sum c(H(x), y) \\\\\n",
    "c(H(x), y) &= \\begin{cases} -log(H(x)) \\quad y = 1 \\\\ -log(1 - H(x)) \\quad y = 0 \\end{cases} \\\\\n",
    "c(H(x), y) &= -y \\cdot log(H(x)) - (1-y) \\cdot log(1-H(x))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize cost - Gradient descent algorithm\n",
    "\n",
    "- Gradient descent\n",
    " - $\\alpha$: learning rate\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "c(W) =& -\\frac{1}{m} \\sum y \\cdot log(H(x)) + (1-y) \\cdot log(1-H(x)) \\\\\n",
    "W :=& W - \\alpha \\frac{\\partial}{\\partial W} cost(W)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- code\n",
    "\n",
    "```\n",
    "# cost function\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis)))\n",
    "\n",
    "# Minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5. Logistic ( regression ) classification\n",
    "---\n",
    "- $H(X) = \\frac{1}{1+e^{-W^T X}}$\n",
    "- $c(W) = -\\frac{1}{m} \\sum y \\cdot log(H(x)) + (1-y) \\cdot log(1-H(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.06748 [[ 0.48464769  0.91838366 -0.7550177 ]]\n",
      "100 0.521366 [[-0.79896683 -0.07749598  0.42639631]]\n",
      "200 0.42402 [[-1.75914693 -0.05038413  0.63074201]]\n",
      "300 0.367072 [[-2.50094962  0.00815814  0.74847698]]\n",
      "400 0.329799 [[-3.10139132  0.05333814  0.84540093]]\n",
      "500 0.30341 [[-3.6065352   0.08720482  0.93049532]]\n",
      "600 0.283586 [[-4.04421854  0.11325153  1.00694621]]\n",
      "700 0.268007 [[-4.43207598  0.13380995  1.0766983 ]]\n",
      "800 0.255328 [[-4.78182936  0.15038267  1.14111257]]\n",
      "900 0.244721 [[-5.10160685  0.16396967  1.20118403]]\n",
      "1000 0.235649 [[-5.39723778  0.17526054  1.25765765]]\n",
      "1100 0.227747 [[-5.67304182  0.18474858  1.31110489]]\n",
      "1200 0.220761 [[-5.93228388  0.19279398  1.36197162]]\n",
      "1300 0.214507 [[-6.17750168  0.19966963  1.41061103]]\n",
      "1400 0.208848 [[-6.4106884   0.20558295  1.45730782]]\n",
      "1500 0.203681 [[-6.63344526  0.21069677  1.50229418]]\n",
      "1600 0.198927 [[-6.84707594  0.21514028  1.54576254]]\n",
      "1700 0.194523 [[-7.05264521  0.2190166   1.58787191]]\n",
      "1800 0.190418 [[-7.25104141  0.22240938  1.628757  ]]\n",
      "1900 0.186574 [[-7.44300842  0.2253883   1.6685313 ]]\n",
      "2000 0.182958 [[-7.62917566  0.22801028  1.7072922 ]]\n",
      "2100 0.179542 [[-7.81008101  0.23032328  1.74512386]]\n",
      "2200 0.176304 [[-7.98618269  0.23236635  1.78209865]]\n",
      "2300 0.173225 [[-8.15787888  0.23417427  1.81827962]]\n",
      "2400 0.170289 [[-8.32551765  0.23577587  1.85372269]]\n",
      "2500 0.167482 [[-8.48939991  0.23719573  1.88847649]]\n",
      "2600 0.164794 [[-8.64979267  0.23845512  1.9225843 ]]\n",
      "2700 0.162212 [[-8.80692577  0.23957223  1.95608413]]\n",
      "2800 0.15973 [[-8.9610157   0.2405639   1.98901117]]\n",
      "2900 0.157338 [[-9.11224174  0.24144335  2.02139544]]\n",
      "3000 0.15503 [[-9.26077366  0.24222295  2.05326557]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "xy = np.loadtxt('./data/train_logistic.txt', unpack=True, dtype='float32')\n",
    "x_data = xy[0:-1]\n",
    "y_data = xy[-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "# Hypothesis\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "\n",
    "# Cost func\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "# Minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# fitting\n",
    "for step in range(3001):\n",
    "    sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask to ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.88748 [[-0.3623485  -0.24675807 -0.41658551]]\n",
      "100 0.462074 [[-1.34712648 -0.06644846  0.54792029]]\n",
      "200 0.390255 [[-2.17783165 -0.01678387  0.69671303]]\n",
      "300 0.345374 [[-2.83659554  0.03409148  0.80205745]]\n",
      "400 0.314653 [[-3.381675    0.07263944  0.89218622]]\n",
      "500 0.292156 [[-3.84801102  0.10195407  0.97236824]]\n",
      "600 0.274816 [[-4.25726557  0.12483323  1.04503465]]\n",
      "700 0.260917 [[-4.62353039  0.14310825  1.11178637]]\n",
      "800 0.249429 [[-4.956388    0.15798049  1.17376947]]\n",
      "900 0.239698 [[-5.2626195   0.17026654  1.23183393]]\n",
      "1000 0.23129 [[-5.54717159  0.18054026  1.28662479]]\n",
      "1100 0.223905 [[-5.81375217  0.18921727  1.33864117]]\n",
      "1200 0.217331 [[-6.06520605  0.19660683  1.38827586]]\n",
      "1300 0.21141 [[-6.30376148  0.20294473  1.43584323]]\n",
      "1400 0.206026 [[-6.53118563  0.20841216  1.48159826]]\n",
      "1500 0.201088 [[-6.74890995  0.21315266  1.52575004]]\n",
      "1600 0.196528 [[-6.9581027   0.21728082  1.5684725 ]]\n",
      "1700 0.19229 [[-7.15973186  0.2208894   1.60991061]]\n",
      "1800 0.18833 [[-7.35460043  0.2240523   1.65018845]]\n",
      "1900 0.184611 [[-7.54338932  0.22683372  1.68940878]]\n",
      "2000 0.181105 [[-7.72667599  0.22928458  1.72766232]]\n",
      "2100 0.177787 [[-7.90495443  0.23144819  1.76502645]]\n",
      "2200 0.174636 [[-8.07865143  0.23336184  1.80156875]]\n",
      "2300 0.171636 [[-8.24813366  0.23505592  1.83734798]]\n",
      "2400 0.168771 [[-8.41372681  0.23655733  1.87241638]]\n",
      "2500 0.166029 [[-8.57570934  0.23788893  1.90681911]]\n",
      "2600 0.163398 [[-8.73433304  0.23907027  1.94059753]]\n",
      "2700 0.160871 [[-8.88981247  0.2401181   1.97378683]]\n",
      "2800 0.158438 [[-9.04234695  0.24104819  2.00641942]]\n",
      "2900 0.156092 [[-9.19211006  0.2418727   2.03852487]]\n",
      "3000 0.153827 [[-9.33925819  0.24260293  2.07012963]]\n",
      "--------------------------------------------------\n",
      "[[False]]\n",
      "[[ True]]\n",
      "[[False  True]]\n"
     ]
    }
   ],
   "source": [
    "# hypothesis\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "\n",
    "# cost func\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "# minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# launch\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# fitting\n",
    "for step in range(3001):\n",
    "    sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "# predict\n",
    "print(sess.run(hypothesis, feed_dict={X:[[1], [2], [2]]})>0.5)\n",
    "print(sess.run(hypothesis, feed_dict={X:[[1], [5], [5]]})>0.5)\n",
    "print(sess.run(hypothesis, feed_dict={X:[[1, 1], [4, 3], [3, 5]]})>0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
