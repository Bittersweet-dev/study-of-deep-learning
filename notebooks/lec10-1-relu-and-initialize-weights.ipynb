{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lecture 10-1. Neural Network 2: ReLu, Initialize weights\n",
    "\n",
    "- ReLU: Better non-linearity\n",
    "- Initialize weights in a smart way\n",
    "- NN dropout and model ensemble\n",
    "- NN LEGO Play\n",
    "- Lab 10. NN, ReLu, Xavier, Dropout, and Adam\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## NN for XOR review\n",
    "![](./img/09-xor-problem-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##  More Complex NN\n",
    "\n",
    "### 3 layers NN\n",
    "- 입력 $X$ 가 2개, 출력 5개 --- Input layer\n",
    "- 그 다음 입력 5개, 출력 4개 --- Hidden layer\n",
    "- 그 다음 입력 4개, 마지막 출력 $y$ 1개 --- Output layer\n",
    "![](./img/10-relu-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 9 hidden layers\n",
    "- 11 개 weights\n",
    "- Input 2x5\n",
    "- Hidden 아무렇게나\n",
    "- Output 5x1\n",
    "![](./img/10-relu-02.png)\n",
    "\n",
    "- TensorBoard\n",
    "![](./img/10-relu-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Poor result\n",
    "- layer 가 9단으로 돌렸는데도 스코어가 낮아짐\n",
    "- cost 가 0.64 에서 줄지 않음\n",
    "- accuracy 도 0.75 까지 갔다가 0.5로 유지됨\n",
    "![](./img/10-relu-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Vanishing gradient (  NN winter 2: 1986-2006 )\n",
    "- Backpropagation : Output layer 에서 Input layer 로 chain rule 을 적용하여 미분해 나감\n",
    "- sigmoid 를 지나면 실수범위의 입력값$(X)$도 $0$~$1$ 까지의 범위로 제한되어 출력됨$(y)$\n",
    "- Backpropagation 은 미분값과 출력값($0$~$1$)의 곱의 연속이기 때문에 layer 가 깊어질 수록 거의 0으로 수렵하게 됨\n",
    " - $w_1x + b_1 = y_1 -> w_2y_1 + b_2 = y_2 ...$\n",
    "![](./img/10-relu-05.png)\n",
    "![](./img/10-relu-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Geoffrey Hinton's summary of findings up to today\n",
    "- Our labeled datasets were thousands of times too small.\n",
    "- Ourcomputers were millions of times too slow.\n",
    "- We initialized the weights in a stupid way.\n",
    "- **We used the wrong type of non-linearity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ReLU: Rectified Linear Unit\n",
    "- Vanishing gradient 문제 해결을 위해 sigmoid 대신 relu 를 사용\n",
    "- $0$ 보다 작을 때에는 $0$ 출력, 클때는 선형 증가로 출력\n",
    "- `L1 = tf.nn.relu(tf.matmul(X, W1) + b1)`\n",
    "![](./img/10-relu-07.png)\n",
    "![](./img/10-relu-08.png)\n",
    "\n",
    "### 성능개선\n",
    "![](./img/10-relu-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 여러가지 Activation Functions\n",
    "- sigmoid 는 안씀\n",
    "- ReLU, Leaky ReLU 를 자주 씀\n",
    "![](./img/10-relu-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Initialize Weights in a smart way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Geoffrey Hinton's summary of findings up to today\n",
    "- Our labeled datasets were thousands of times too small.\n",
    "- Ourcomputers were millions of times too slow.\n",
    "- **We initialized the weights in a stupid way.**\n",
    "- We used the wrong type of non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 같은 ReLu 라도 초기 weights 값에 따라 성능개선 속도 다름\n",
    "- If Set all initial weights to 0\n",
    " - Backpropagation 하면서 나갈때 앞에있는 모든 기울기기 0 이 됨\n",
    " \n",
    "![](./img/10-relu-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Need to set the initial weight values wisely\n",
    "- Not all 0's\n",
    "- Challenging issue\n",
    "- Hinton et al. (2006) \"A Fast Learning Algorithm for Deep Belief Nets\"\n",
    " - Restricted Boatman Machine ( RBM )\n",
    "\n",
    "### RBM\n",
    "- Initial weights 을 설정하기 위한 알고리즘\n",
    " - 2 layers 간의 계산만 함\n",
    " - input 레이어의 $X$ 값으로 forward 진행\n",
    " - output 레이어로부터 $\\hat{X}$ backpropagation 진행\n",
    " - $X = \\hat{X}$ 가 같도록 초기 $W$ 값을 설정\n",
    "![](./img/10-init-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### How can we use RBM to initialize weights ?\n",
    "- Apply the RBM idea on adjacent two layers as a pre-training step\n",
    "- Continue the firtst process to all layers\n",
    "- This will set weights\n",
    "- Example: Deep Belief Network\n",
    " - Weight initialized by RBM\n",
    "![](./img/10-init-02.png)\n",
    "![](./img/10-init-03.png)\n",
    "![](./img/10-init-04.png)\n",
    "![](./img/10-init-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Good news\n",
    "- No need to use complicated RBM for weight initializations\n",
    "- SImple methods are OK\n",
    " - **Xavier initialization**: X.Glorot and Y.Bengio, \"Understanding the difficulty of training deep feedforward neural networks\" in International conference on artificial intelligence and statistics, 2010\n",
    " - **He's initialization**: K.He, X.Zhang, S.Ren, and J.Sun, \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Xacier/He initialization\n",
    "- Makes sure the weights are 'just right', not too small, not too big\n",
    "- Using number of input ( fan_in ) and output ( fan_out )\n",
    "\n",
    "```\n",
    "# Xavier initialization\n",
    "# Glorot et al. 2010\n",
    "W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)\n",
    "\n",
    "# He et al. 2015\n",
    "W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Still an active area of research\n",
    "- We don't know how to initialize perfect weight values, yet\n",
    "- Many new algorithms\n",
    " - Batch normalization\n",
    " - Layer sequential uniform variance\n",
    " - ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
