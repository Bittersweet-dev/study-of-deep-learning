{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lecture 6. Softmax classification - Multinomial classfication\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "- Logistic function\n",
    "- Sigmoid function\n",
    " - sigmoid is curved in two directions, like the letter \"S\", or the Greek $\\varsigma$ (sigma)\n",
    "$$g(z) = \\frac{1}{(1 + e^{-z})}$$\n",
    "\n",
    "### Hypothesis\n",
    "$$\n",
    "\\begin{align}\n",
    "H(x) &= g(z) \\\\\n",
    "( z &= WX )\\\\\n",
    "H(X) &= \\frac{1}{1+e^{-W^T X}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Multinomial classification\n",
    "\n",
    "### OvR ( One vs Rest )\n",
    "- A, B, C 세 가지 class 가 존재하는 분류 문제의 경우\n",
    " - A 인지 아닌지, \n",
    " - B 인지 아닌지, \n",
    " - C 인지 아닌지 분류하는 3개의 classifier 를 가지고 구현 가능하다.\n",
    " \n",
    "### Matrix form\n",
    "\n",
    "![](./img/06-softmax01.png?raw=true)\n",
    "\n",
    "![](./img/06-softmax02.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Softmax - hypothesis\n",
    "![](./img/06-softmax03.png?raw=true)\n",
    "![](./img/06-softmax04.png?raw=true)\n",
    "![](./img/06-softmax05.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cost function\n",
    "- L : label ( 실제값 )\n",
    "- S : sigmoid ( 예측값 )\n",
    "\n",
    "![](./img/06-softmax06.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### cross entropy cost function\n",
    "- $-\\sum L_i \\log{S_i} = -\\sum L_i \\log{\\hat{y_i}} = \\sum L_i \\odot (-\\log{\\hat{y_i})}$\n",
    "- $0 \\le \\hat{y_i} \\le 1$ -> $+\\infty \\le -\\log{\\hat{y_i}} \\le 0$\n",
    "\n",
    "![](./img/06-softmax07.png?raw=true)\n",
    "\n",
    "- **ex.** $L = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = B : 실제$\n",
    " - $\\hat{y} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = B : 정답$ -> $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\odot - \\log \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\odot \\begin{bmatrix} \\infty \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 0$\n",
    " - $\\hat{y} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = A : 오답$ -> $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\odot - \\log \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\odot \\begin{bmatrix} 0 \\\\ \\infty \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\infty \\end{bmatrix} = \\infty$\n",
    " \n",
    "- **ex.** $L = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = A : 실제$\n",
    " - $\\hat{y} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = A : 정답$ -> $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\odot \\begin{bmatrix} 0 \\\\ \\infty \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 0$\n",
    " - $\\hat{y} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = A : 오답$ -> $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\odot \\begin{bmatrix} \\infty \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\infty \\\\ 0 \\end{bmatrix} = \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic cost VS cross entropy\n",
    "\n",
    "- 둘다 같음\n",
    " - Logistic cost\n",
    "   - $c(H(x), y) = -y \\cdot log(H(x)) - (1-y) \\cdot log(1-H(x))$\n",
    " - cross entropy\n",
    "   - $D(S, L) = \\sum L_i \\log(S_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cost function\n",
    "- with training set\n",
    "\n",
    "![](./img/06-softmax08.png?raw=true)\n",
    "\n",
    "- $\\mathcal{L} : Loss$\n",
    "- $D(w_1, w_2) : Distance$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradient descent\n",
    "- Step : $\\alpha \\times derivative = -\\alpha \\times \\Delta \\mathcal{L}(w_1, w_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 6. Softmax classifier\n",
    "---\n",
    "- `tf.matmul(W, X)`\n",
    "- `hypothesis = tf.nn.softmax(tf.matmul(W, X))`\n",
    "- `cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), reduction_indices=1))`\n",
    "- `optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.73609 [[-0.00066667  0.00033333  0.00033333]\n",
      " [ 0.00133333  0.00233333 -0.00366667]\n",
      " [ 0.00133333  0.00333333 -0.00466667]]\n",
      "500 7.40905 [[-0.41474834 -0.10312591  0.51787436]\n",
      " [ 0.03761362 -0.10297523  0.06536176]\n",
      " [ 0.07454177  0.17755088 -0.2520926 ]]\n",
      "1000 6.7281 [[-0.76251483 -0.19494899  0.95746374]\n",
      " [ 0.05222586 -0.12449741  0.07227183]\n",
      " [ 0.13092487  0.22215752 -0.35308245]]\n",
      "1500 6.24793 [[-1.0621376  -0.26723006  1.3293674 ]\n",
      " [ 0.06807303 -0.11824042  0.05016781]\n",
      " [ 0.17547619  0.23513761 -0.41061375]]\n",
      "2000 5.88735 [[-1.3272748  -0.32214603  1.64941895]\n",
      " [ 0.08332979 -0.10558683  0.02225749]\n",
      " [ 0.21334152  0.23823248 -0.45157382]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('./data/train_softmax.txt', unpack=True, dtype='float32')\n",
    "x_data = np.transpose(xy[0:3])\n",
    "y_data = np.transpose(xy[3:])\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\", [None, 3]) # x1, x2 and 1 ( for bias ) # None : 데이터의 sample 개수 모르기떄문에 None 으로 부여함\n",
    "y = tf.placeholder(\"float\", [None, 3]) # A, B, C => 3 classes\n",
    "\n",
    "# Set model wights\n",
    "W = tf.Variable(tf.zeros([3, 3])) # W : 3 x 3 ( label 이 3개, feature 가 3개 )\n",
    "\n",
    "# Construct model\n",
    "\"\"\"\n",
    "https://www.tensorflow.org/tutorials/mnist/beginners/\n",
    "First, we multiply x by W with the expression tf.matmul(x, W). \n",
    "This is flipped from when we multiplied them in our equation, \n",
    "where we had Wx, as a small trick to deal with x being a 2D tensor with multiple inputs. \n",
    "We then add b, and finally apply tf.nn.softmax.\n",
    "\"\"\"\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W)) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis)))\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.73609 [[-0.00066667  0.00033333  0.00033333]\n",
      " [ 0.00133333  0.00233333 -0.00366667]\n",
      " [ 0.00133333  0.00333333 -0.00466667]]\n",
      "500 7.40905 [[-0.41474834 -0.10312591  0.51787436]\n",
      " [ 0.03761362 -0.10297523  0.06536176]\n",
      " [ 0.07454177  0.17755088 -0.2520926 ]]\n",
      "1000 6.7281 [[-0.76251483 -0.19494899  0.95746374]\n",
      " [ 0.05222586 -0.12449741  0.07227183]\n",
      " [ 0.13092487  0.22215752 -0.35308245]]\n",
      "1500 6.24793 [[-1.0621376  -0.26723006  1.3293674 ]\n",
      " [ 0.06807303 -0.11824042  0.05016781]\n",
      " [ 0.17547619  0.23513761 -0.41061375]]\n",
      "2000 5.88735 [[-1.3272748  -0.32214603  1.64941895]\n",
      " [ 0.08332979 -0.10558683  0.02225749]\n",
      " [ 0.21334152  0.23823248 -0.45157382]]\n",
      "[[ 0.66555417  0.27094138  0.06350452]] [0]\n",
      "[[ 0.25935882  0.44414687  0.29649431]] [1]\n",
      "[[ 0.04603587  0.10412923  0.84983486]] [2]\n",
      "[[ 0.66555417  0.27094138  0.06350452]\n",
      " [ 0.25935882  0.44414687  0.29649431]\n",
      " [ 0.04603587  0.10412923  0.84983486]] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "xy = np.loadtxt(\"./data/train_softmax.txt\", dtype=\"float32\")\n",
    "x_data = xy[:, 0:3]\n",
    "y_data = xy[:, 3:]\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "# Set model weight\n",
    "W = tf.Variable(tf.zeros([3, 3]))\n",
    "\n",
    "# Constructor model\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W)) # Softmax\n",
    "\n",
    "# cost function ( cross entropy )\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis)))\n",
    "\n",
    "# minimize \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, y: y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, y: y_data}), sess.run(W))\n",
    "    \n",
    "    # Predict\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7]]})\n",
    "    print(a, sess.run(tf.arg_max(a, 1)))\n",
    "    \n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4]]})\n",
    "    print(b, sess.run(tf.arg_max(b, 1)))\n",
    "    \n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0]]})\n",
    "    print(c, sess.run(tf.arg_max(c, 1)))\n",
    "    \n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7], [1, 3, 4], [1, 1, 0]]})\n",
    "    print(all, sess.run(tf.arg_max(all, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
