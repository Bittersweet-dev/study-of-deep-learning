{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 10. NN, ReLu, Xavier, Dropout and Adam\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost= 0.521184075\n",
      "Epoch: 0002, Cost= 0.352593903\n",
      "Epoch: 0003, Cost= 0.324264894\n",
      "Epoch: 0004, Cost= 0.309338853\n",
      "Epoch: 0005, Cost= 0.299760784\n",
      "Epoch: 0006, Cost= 0.292863023\n",
      "Epoch: 0007, Cost= 0.287666024\n",
      "Epoch: 0008, Cost= 0.283491803\n",
      "Epoch: 0009, Cost= 0.280248994\n",
      "Epoch: 0010, Cost= 0.277030452\n",
      "Epoch: 0011, Cost= 0.274587417\n",
      "Epoch: 0012, Cost= 0.272341755\n",
      "Epoch: 0013, Cost= 0.270360379\n",
      "Epoch: 0014, Cost= 0.268654794\n",
      "Epoch: 0015, Cost= 0.266825286\n",
      "Accuracy: 0.9218\n"
     ]
    }
   ],
   "source": [
    "# tf graph input\n",
    "X = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float', [None, 10])\n",
    "\n",
    "# set weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# model\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# minimize cost\n",
    "learning_rate = 0.1\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis), reduction_indices=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# set training_epochs, batch_size, display_step\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict = {X:batch_xs, y:batch_ys})\n",
    "            avg_cost += sess.run(cost, feed_dict={X:batch_xs, y:batch_ys}) / total_batch\n",
    "        # Display logs\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch: {:04d}, Cost= {:.9f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sfotmax with tensorbard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost= 0.922257241\n",
      "Epoch: 0002, Cost= 0.524568467\n",
      "Epoch: 0003, Cost= 0.452979745\n",
      "Epoch: 0004, Cost= 0.417778888\n",
      "Epoch: 0005, Cost= 0.395971195\n",
      "Epoch: 0006, Cost= 0.380602818\n",
      "Epoch: 0007, Cost= 0.368948672\n",
      "Epoch: 0008, Cost= 0.359839258\n",
      "Epoch: 0009, Cost= 0.352355014\n",
      "Epoch: 0010, Cost= 0.346042725\n",
      "Epoch: 0011, Cost= 0.340726275\n",
      "Epoch: 0012, Cost= 0.336137661\n",
      "Epoch: 0013, Cost= 0.331947731\n",
      "Epoch: 0014, Cost= 0.328308926\n",
      "Epoch: 0015, Cost= 0.325056534\n",
      "Accuracy: 0.910291\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, 784], name = \"X-input\")\n",
    "y = tf.placeholder(\"float\", [None, 10], name = \"y-input\")\n",
    "\n",
    "# Set weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "# Add histogram\n",
    "w_hist = tf.summary.histogram(\"weight\", W)\n",
    "b_hist = tf.summary.histogram(\"bias\", b)\n",
    "y_hist = tf.summary.histogram(\"y\", y)\n",
    "\n",
    "\n",
    "# Construct model\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Minimize Cost\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis), reduction_indices=1))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # tensorboard merge\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/softmax_mnist_logs\", sess.graph)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={X:batch_xs, y:batch_ys})\n",
    "            avg_cost += sess.run(cost, feed_dict={X:batch_xs, y:batch_ys}) / total_batch\n",
    "            \n",
    "            # tensorbard\n",
    "            if epoch % display_step == 0:\n",
    "                summary, _ = sess.run([merged, optimizer], feed_dict={X:batch_xs, y:batch_ys})\n",
    "                writer.add_summary(summary, epoch)\n",
    "            \n",
    "        # Display logs\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch: {:04d}, Cost= {:.9f}\".format(epoch+1, avg_cost))\n",
    "        \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:mnist.train.images, y:mnist.train.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NN ( Neural Network ) with ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, cost= 170.300113751\n",
      "epoch: 0002, cost= 39.842550096\n",
      "epoch: 0003, cost= 24.403118906\n",
      "epoch: 0004, cost= 16.874933719\n",
      "epoch: 0005, cost= 12.071800723\n",
      "epoch: 0006, cost= 8.843322552\n",
      "epoch: 0007, cost= 6.461736077\n",
      "epoch: 0008, cost= 4.745275340\n",
      "epoch: 0009, cost= 3.399168718\n",
      "epoch: 0010, cost= 2.416047792\n",
      "epoch: 0011, cost= 1.665859365\n",
      "epoch: 0012, cost= 1.192046224\n",
      "epoch: 0013, cost= 0.837875080\n",
      "epoch: 0014, cost= 0.573309850\n",
      "epoch: 0015, cost= 0.452114851\n",
      "Optimization Finished !!!\n",
      "Accuracy: 0.989309\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float', [None, 10])\n",
    "\n",
    "# Set weights & bias\n",
    "W1 = tf.Variable(tf.random_normal([784, 256])) # tf.zeros() 로 초기값 주면 망함 accuracy = 0.11\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # Hidden layers with ReLu activation\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "hypothesis = tf.matmul(L2, W3) + b3 # No need to use softmax here\n",
    "\n",
    "# Minimize Cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Loop over all epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={X:batch_xs, y:batch_ys})\n",
    "            avg_cost += sess.run(cost, feed_dict={X:batch_xs, y:batch_ys}) / total_batch\n",
    "            \n",
    "        # Display logs\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"epoch: {:04d}, cost= {:.9f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "    print(\"Optimization Finished !!!\")\n",
    "    \n",
    "    # Test\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:mnist.train.images, y:mnist.train.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NN with Xavier Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, cost= 0.291347500\n",
      "epoch: 0002, cost= 0.097839827\n",
      "epoch: 0003, cost= 0.061555574\n",
      "epoch: 0004, cost= 0.041461504\n",
      "epoch: 0005, cost= 0.029221285\n",
      "epoch: 0006, cost= 0.020642745\n",
      "epoch: 0007, cost= 0.014702703\n",
      "epoch: 0008, cost= 0.011271844\n",
      "epoch: 0009, cost= 0.008376830\n",
      "epoch: 0010, cost= 0.007577952\n",
      "epoch: 0011, cost= 0.005036811\n",
      "epoch: 0012, cost= 0.005082444\n",
      "epoch: 0013, cost= 0.005003386\n",
      "epoch: 0014, cost= 0.003872608\n",
      "epoch: 0015, cost= 0.002858293\n",
      "Optimization Finished !\n",
      "Accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "    \"\"\"Set the parameter initialization using the method described.\n",
    "    This method is designed to keep the scale of the gradients roughly the same\n",
    "    in all layers.\n",
    "    Xavier Glorot and Yoshua Bengio (2010):\n",
    "            Understanding the difficulty of training deep feedforward neural\n",
    "            networks. International conference on artificial intelligence and\n",
    "            statistics.\n",
    "    Args:\n",
    "      n_inputs: The number of input nodes into each output.\n",
    "      n_outpus: The number of output nodes for each input.\n",
    "      uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "    Returns:\n",
    "      An initializer.\n",
    "    \"\"\"\n",
    "    if uniform:\n",
    "        # 6 was used in the paper\n",
    "        init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        # 3 gives us approximately the smae limits as above since this repicks\n",
    "        # values greater than 2 standard deviations from the maen.\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)\n",
    "    \n",
    "\n",
    "# Set parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float', [None, 10])\n",
    "\n",
    "# Set weights & biases\n",
    "with tf.variable_scope(\"weights1\") as scope:\n",
    "    try:\n",
    "        W1 = tf.get_variable('W1', shape=[784, 256], initializer=xavier_init(784, 256)) # get_variable\n",
    "        W2 = tf.get_variable('W2', shape=[256, 256], initializer=xavier_init(256, 256))\n",
    "        W3 = tf.get_variable('W3', shape=[256, 10], initializer=xavier_init(256, 10))\n",
    "    except ValueError:\n",
    "        scope.reuse_variables()\n",
    "        W1 = tf.get_variable('W1', shape=[784, 256], initializer=xavier_init(784, 256)) \n",
    "        W2 = tf.get_variable('W2', shape=[256, 256], initializer=xavier_init(256, 256))\n",
    "        W3 = tf.get_variable('W3', shape=[256, 10], initializer=xavier_init(256, 10))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# Minimize cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all total_batch\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={X:batch_xs, y:batch_ys})\n",
    "            avg_cost += sess.run(cost, feed_dict={X:batch_xs, y:batch_ys}) / total_batch\n",
    "        # Display logs\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"epoch: {:04d}, cost= {:.9f}\".format(epoch+1, avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished !\")\n",
    "            \n",
    "    # Test\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NN with Deeper & Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Xavier Initializer\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "    \"\"\"Set the parameter initialization using the method described.\n",
    "    This method is designed to keep the scale of the gradients roughly the same\n",
    "    in all layers.\n",
    "    Xavier Glorot and Yoshua Bengio (2010):\n",
    "            Understanding the difficulty of training deep feedforward neural\n",
    "            networks. International conference on artificial intelligence and\n",
    "            statistics.\n",
    "    Args:\n",
    "      n_inputs: The number of input nodes into each output.\n",
    "      n_outpus: The number of output nodes for each input.\n",
    "      uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "    Returns:\n",
    "      An initializer.\n",
    "    \"\"\"\n",
    "    if uniform:\n",
    "        # 6 was used in the paper\n",
    "        init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        # 3 gives us approximately the smae limits as above since this repicks\n",
    "        # values greater than 2 standard deviations from the maen.\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, cost=0.614708607\n",
      "epoch: 0002, cost=0.218250300\n",
      "epoch: 0003, cost=0.162101555\n",
      "epoch: 0004, cost=0.136621421\n",
      "epoch: 0005, cost=0.117607634\n",
      "epoch: 0006, cost=0.106442019\n",
      "epoch: 0007, cost=0.095276398\n",
      "epoch: 0008, cost=0.088351584\n",
      "epoch: 0009, cost=0.079378287\n",
      "epoch: 0010, cost=0.074049040\n",
      "epoch: 0011, cost=0.070707715\n",
      "epoch: 0012, cost=0.065686001\n",
      "epoch: 0013, cost=0.065092079\n",
      "epoch: 0014, cost=0.058808897\n",
      "epoch: 0015, cost=0.055060618\n",
      "Optimization Finished !\n",
      "Accuaracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf graph input\n",
    "X = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float', [None, 10])\n",
    "\n",
    "# Store layers weights & biases\n",
    "with tf.variable_scope(\"weights2\") as scope:\n",
    "    try:\n",
    "        W1 = tf.get_variable('W1', shape=[784, 256], initializer=xavier_init(784, 256))\n",
    "        W2 = tf.get_variable('W2', shape=[256, 256], initializer=xavier_init(256, 256))\n",
    "        W3 = tf.get_variable('W3', shape=[256, 256], initializer=xavier_init(256, 256))\n",
    "        W4 = tf.get_variable('W4', shape=[256, 128], initializer=xavier_init(256, 128))\n",
    "        W5 = tf.get_variable('W5', shape=[128, 10], initializer=xavier_init(128, 10))\n",
    "    except ValueError:\n",
    "        scope.reuse_variables()\n",
    "        W1 = tf.get_variable('W1', shape=[784, 256], initializer=xavier_init(784, 256))\n",
    "        W2 = tf.get_variable('W2', shape=[256, 256], initializer=xavier_init(256, 256))\n",
    "        W3 = tf.get_variable('W3', shape=[256, 256], initializer=xavier_init(256, 256))\n",
    "        W4 = tf.get_variable('W4', shape=[256, 128], initializer=xavier_init(256, 128))\n",
    "        W5 = tf.get_variable('W5', shape=[128, 10], initializer=xavier_init(128, 10))\n",
    "        \n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "b4 = tf.Variable(tf.random_normal([128]))\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model with dropout\n",
    "dropout_rate = tf.placeholder('float')\n",
    "\n",
    "_L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(_L1, dropout_rate)\n",
    "_L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(_L2, dropout_rate)\n",
    "_L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(_L3, dropout_rate)\n",
    "_L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(_L4, dropout_rate)\n",
    "\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# Minimize cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Trai, Loop over tatal batch\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={X:batch_xs, y:batch_ys, dropout_rate:0.7})\n",
    "            avg_cost += sess.run(cost, feed_dict={X:batch_xs, y:batch_ys, dropout_rate:0.7}) / total_batch\n",
    "        # Display logs    \n",
    "        if epoch % display_step == 0:\n",
    "            print(\"epoch: {:04d}, cost={:.9f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "    print(\"Optimization Finished !\")\n",
    "    \n",
    "    # Test\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    print(\"Accuaracy:\", accuracy.eval({X:mnist.test.images, y:mnist.test.labels, dropout_rate:1}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
