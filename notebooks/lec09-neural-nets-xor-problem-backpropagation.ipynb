{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9. Neural Network 1: XOR 문제와 학습방법, Backpropagation (1986 breakthrough)\n",
    "- XOR 문제 딥러닝으로 풀기\n",
    "- 특별편: 10분안에 미분 정리하기\n",
    "- 딥넷트웍 학습 시키기 (backpropagation)\n",
    "- 실습1: XOR을 위한 텐스플로우 딥넷트웍 \n",
    "- 실습2: Tensor Board로 딥네트웍 들여다보기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One logistic regression unit cannot separate XOR\n",
    "- 하나의 모델로는 XOR 문제를 풀 수 없음\n",
    "\n",
    "### Multiple logistic regression units\n",
    "- 여러개의 모델로 풀 수 있음\n",
    "\n",
    "### Neural Network ( NN )\n",
    "- No one on earth had found a viable way to train\n",
    "- 그러나 수 많은 weights 를 학습시키기 어렵다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR using NN\n",
    "- 예) 3개 Gate 로 XOR 문제 풀기\n",
    "\n",
    "<img src=\"./img/09-xor-problem-01.png\", width=450>\n",
    "<img src=\"./img/09-xor-problem-02.png\", width=450>\n",
    "<img src=\"./img/09-xor-problem-03.png\", width=450>\n",
    "<img src=\"./img/09-xor-problem-04.png\", width=450>\n",
    "\n",
    "### Forward propagation\n",
    "- 위 문제를 하나의 그림으로 표현 가능\n",
    "- Node : Gate, Unit, Perceptron 이라고 불림\n",
    "- Can you find another $w$ and $b$ for the XOR ?\n",
    "<img src=\"./img/09-xor-problem-05.png\", width=450>\n",
    "\n",
    "- 각각의 $w$ vector 를 하나의 matrix 로 표현\n",
    "<img src=\"./img/09-xor-problem-06.png\", width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "- $K(X) = sigmoid({XW}_1 + b_1)$ --- 앞 단의 layer\n",
    "- $\\hat{y} = H(X) = sigmoid({K(X)W}_2 + b_2)$ --- 뒷 단의 layer\n",
    "\n",
    "```\n",
    "# NN\n",
    "K = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "hypothesis = tf.digmoid(tf.matmul(K, W2) + b2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## How can we learn $W_1, W_2, b_1, b_2$ from training data ?\n",
    "\n",
    "### Back propagation ( chain rule )\n",
    "- 뉴럴 네트워크에서 layer 가 많아질 수록 미분 과정은 굉장히 복잡함\n",
    " - 각각의 node 에서 weight의 미분을 구해야함 ( 미분 = 영향력, 기울기, 변화량 )\n",
    "- Backpropagation 1974, 1982 by Paul Werbos, 1986 by Hinton\n",
    " - 실제값과 예측값을 비교해서 나오는 error = cost 를 구하여\n",
    " - error 를 뒤에서 부터 앞으로 보내면서 미분값을 역으로 구하는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제를 통해 Backpropagation 이해하기\n",
    "\n",
    "### 1. forward \n",
    "- $w = -2, x = 5, b = 3$ 집어 넣어 계산\n",
    "\n",
    "### 2. backward\n",
    "- $w$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial w}$ \n",
    "- $x$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial x}$ \n",
    "- $b$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial b}$ \n",
    "<img src=\"./img/09-backpropagation-01.png\", width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미분값의 의미\n",
    "- $g$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial g} = \\frac{\\partial }{\\partial g} (g + b) =  1$\n",
    "- $b$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial g} = \\frac{\\partial }{\\partial b} (g + b) =  1$\n",
    "- $w$ 가 $g$ 에 미치는 영향 : $\\frac{\\partial g}{\\partial w} = \\frac{\\partial }{\\partial w} (wx) =  x$\n",
    "- $x$ 가 $g$ 에 미치는 영향 : $\\frac{\\partial g}{\\partial x} = \\frac{\\partial }{\\partial x} (wx) =  w$\n",
    "\n",
    "<img src=\"./img/09-backpropagation-02.png\", width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미분값의 의미\n",
    "- $\\frac{\\partial f}{\\partial w} = 5$ 라는 것은 $w$ 가 변함에 따라 $f$ 는 5배 변함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 더 많은 layer 에서도 적용 가능\n",
    "- 아무리 많은 node 를 갖더라도 뒤에서 부터 chain rule 을 적용하면 모든 미분값을 구할 수 있음\n",
    "<img src=\"./img/09-backpropagation-03.png\", width=450>\n",
    "\n",
    "### sigmoid 미분도 같은 원리로 진행 가능\n",
    "<img src=\"./img/09-backpropagation-04.png\", width=450>\n",
    "\n",
    "### TensorBoard\n",
    "- TensorFlow 에서 Backporpagation 과정을 보여주는 그래프\n",
    "<img src=\"./img/09-backpropagation-05.png\", width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9. NN for XOR & TensorBoard\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set\n",
    "```\n",
    "# XOR\n",
    "# x1 x2 y\n",
    "0 0 0\n",
    "0 1 1\n",
    "1 0 1\n",
    "1 1 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR with Logistic regression\n",
    "- Does not work !\n",
    "- accuracy is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.802896 [[-0.5700044 ]\n",
      " [-0.99632156]]\n",
      "200 0.751598 [[-0.36781552]\n",
      " [-0.749044  ]]\n",
      "400 0.723087 [[-0.22215967]\n",
      " [-0.56094623]]\n",
      "600 0.708261 [[-0.12186444]\n",
      " [-0.42190552]]\n",
      "800 0.700821 [[-0.05518708]\n",
      " [-0.32044441]]\n",
      "[array([[ 0.5       ],\n",
      "       [ 0.43859547],\n",
      "       [ 0.49689505],\n",
      "       [ 0.43553969]], dtype=float32), array([[ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]], dtype=float32), array([[False,  True,  True, False],\n",
      "       [ True, False, False,  True],\n",
      "       [ True, False, False,  True],\n",
      "       [ True, False, False,  True]], dtype=bool), 0.5]\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "''' \n",
    "# same code\n",
    "xy = np.loadtxt('./data/train_xor.txt', unpack=True)\n",
    "x_data = xy[0:-1]\n",
    "y_data = xy[-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "# Hypothesis\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "'''\n",
    "\n",
    "xy = np.loadtxt('./data/train_xor.txt')\n",
    "x_data = xy[:,:-1]\n",
    "y_data = xy[:,-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "# Hypothesis\n",
    "h = tf.matmul(X, W)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "\n",
    "# Cost\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "# Minimize\n",
    "a = tf.Variable(0.01) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Fit the line\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W))\n",
    "            \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y) # floor 1의자리 수 반환\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN for XOR\n",
    "<img src=\"./img/09-xor-problem-06.png\", width=600>\n",
    "\n",
    "- input X is 2 and sample is 4, so matrix X is 4x2\n",
    "- layer 1 is 2 node(output is 2), so weight W1 is 2x2\n",
    "- layer 2 is 1 node(output is 1), so weight W2 is 2x1\n",
    "- output y is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.698871 [[-0.32003805  0.09612964]\n",
      " [ 0.19567896 -0.50163472]] [[-0.17828195]\n",
      " [ 0.67169499]]\n",
      "1000 0.693228 [[-0.29293999  0.13536705]\n",
      " [ 0.17663494 -0.48805195]] [[-0.20377569]\n",
      " [ 0.56123513]]\n",
      "2000 0.693133 [[-0.27539936  0.18263243]\n",
      " [ 0.16064265 -0.49406895]] [[-0.16726148]\n",
      " [ 0.53092581]]\n",
      "3000 0.692998 [[-0.26170829  0.24109216]\n",
      " [ 0.14839529 -0.53161108]] [[-0.13495043]\n",
      " [ 0.51708162]]\n",
      "4000 0.692698 [[-0.25041899  0.32774261]\n",
      " [ 0.13799499 -0.61626273]] [[-0.10477783]\n",
      " [ 0.53087521]]\n",
      "5000 0.691628 [[-0.24029742  0.48533201]\n",
      " [ 0.1279088  -0.79856122]] [[-0.074985  ]\n",
      " [ 0.61485952]]\n",
      "[array([[ 0.50441968],\n",
      "       [ 0.47899354],\n",
      "       [ 0.52373141],\n",
      "       [ 0.49421316]], dtype=float32), array([[ 1.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[False],\n",
      "       [False],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool), 0.5]\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt(\"./data/train_xor.txt\")\n",
    "x_data = xy[:, 0:-1] # 4x2\n",
    "y_data = xy[:, -1].reshape(4, 1) # 4x1 ( o ) / 1x4 로 했더니 test 에서 문제발생\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]), name=\"Bias1\") # node 2개 -> bias 도 2개\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\") # node 1개 -> bias 도 1개\n",
    "\n",
    "# hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1) # layer 1의 출력 = layer 2의 입력\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2) # L2 를 입력으로 하는 sigmoid\n",
    "\n",
    "# cost\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1.-hypothesis))\n",
    "\n",
    "# minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Fitting\n",
    "    for step in range(5001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W1), sess.run(W2))\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y)\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide NN for XOR\n",
    "- layer 1 is 2 x 10\n",
    "- layer 2 is 10 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.701737 [[-0.36192796 -0.01094407  0.08593327  0.62643492  0.86265218  0.01789026\n",
      "  -0.66106474  0.49977496 -0.28832582 -0.19430409]\n",
      " [-0.25885952  0.54941255  0.86947858 -0.71415263 -0.87108994  0.59894454\n",
      "   0.23578243 -0.56382507  0.34987813  0.24381126]] [[-0.76443636]\n",
      " [-0.55339724]\n",
      " [ 0.51464546]\n",
      " [ 0.57149988]\n",
      " [ 0.68187207]\n",
      " [-0.99276406]\n",
      " [ 0.3231549 ]\n",
      " [ 0.33404908]\n",
      " [ 0.3126474 ]\n",
      " [ 0.00120686]]\n",
      "5000 0.0363677 [[-3.62976122  0.8310594   3.124614    0.60912269  4.34235144  1.5357275\n",
      "  -4.8666172   1.32822406 -1.71197617  0.18390752]\n",
      " [-3.76308799  0.66795337  3.32358527 -1.64036965 -5.55177641  0.26524001\n",
      "   3.43191385 -0.23368432  0.50555009  0.54510432]] [[-4.40436363]\n",
      " [-1.98467243]\n",
      " [ 2.86867785]\n",
      " [ 0.97112888]\n",
      " [ 7.13362741]\n",
      " [-2.68268633]\n",
      " [ 5.84297562]\n",
      " [-1.35922945]\n",
      " [ 1.33613098]\n",
      " [-0.78125209]]\n",
      "10000 0.0101837 [[-4.1537447   0.93760151  3.51771617  0.78664869  4.99886179  1.81745815\n",
      "  -5.62090397  1.64740705 -2.05561614  0.28127131]\n",
      " [-4.20327282  0.85319602  3.63311386 -1.94600403 -6.29186392  0.35649458\n",
      "   4.14825869 -0.269191    0.66118437  0.73215079]] [[-5.17540407]\n",
      " [-2.42962527]\n",
      " [ 3.26596546]\n",
      " [ 1.36313677]\n",
      " [ 9.02830219]\n",
      " [-3.18055677]\n",
      " [ 7.39054203]\n",
      " [-1.79007947]\n",
      " [ 1.64912295]\n",
      " [-1.11080325]]\n",
      "[array([[ 0.00607421],\n",
      "       [ 0.99068856],\n",
      "       [ 0.98963052],\n",
      "       [ 0.01475357]], dtype=float32), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[ True],\n",
      "       [ True],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool), 1.0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt(\"./data/train_xor.txt\")\n",
    "x_data = xy[:, 0:-1] # 4x2\n",
    "y_data = xy[:, -1].reshape(4, 1) # 4x1 ( o ) / 1x4 로 했더니 test 에서 문제발생\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([10, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10]), name=\"Bias1\") # node 10개 -> bias 도 10개\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\") # node 1개 -> bias 도 1개\n",
    "\n",
    "# hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1) # layer 1의 출력 = layer 2의 입력\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2) # L2 를 입력으로 하는 sigmoid\n",
    "\n",
    "# cost\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1.-hypothesis))\n",
    "\n",
    "# minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Fitting\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 5000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W1), sess.run(W2))\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y)\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN for XOR\n",
    "- layer 1 is 2x5\n",
    "- layer 2 is 5x4\n",
    "- layer 3 is 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.765463 [[ 0.05549578 -0.49081454  0.14112693  0.06794445  0.89579666]\n",
      " [ 0.50206071  0.51195323  0.9873358   0.01502027  0.59977704]] [[-0.43884668  0.82401705 -0.17651528 -0.34451643]\n",
      " [ 0.25826123 -0.23553129  0.50972509  0.06639147]\n",
      " [ 0.17102948  0.71482009  0.0896311  -0.15547925]\n",
      " [-0.1442726  -0.02618709 -0.87892818 -0.12720419]\n",
      " [ 0.36363903  0.85061365  0.44182932 -0.62153703]]\n",
      "5000 0.677821 [[ 0.62836361 -0.48920536  0.57824588  0.01141397  1.50210595]\n",
      " [ 0.87354124  0.63108271  1.21425271 -0.02347011  1.307585  ]] [[-0.45492619  0.88700438 -0.18897001 -0.41944835]\n",
      " [ 0.29956034 -0.44378945  0.54062295  0.15383095]\n",
      " [ 0.09988339  0.88538283  0.03071871 -0.31177929]\n",
      " [-0.13312331 -0.18612573 -0.87009883 -0.06773402]\n",
      " [ 0.0139803   1.32215893  0.15451668 -1.06541395]]\n",
      "10000 0.0262029 [[ 1.91587889 -1.55431986  2.1146276  -2.92726612  4.75218821]\n",
      " [ 2.40035892  3.47387433  1.70748925 -1.32412612  5.34223175]] [[-0.09777078  1.76911306  0.13755338 -0.07676638]\n",
      " [ 1.73595977 -1.16569793  1.96456242  2.10293627]\n",
      " [ 2.07109928  1.67626381  2.1564219   1.5210619 ]\n",
      " [-2.54359937 -1.07690573 -3.71660256 -1.68145335]\n",
      " [-1.57366836  4.76226759 -1.30891526 -3.66731691]]\n",
      "[array([[ 0.02095608],\n",
      "       [ 0.96606266],\n",
      "       [ 0.97999668],\n",
      "       [ 0.02848664]], dtype=float32), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[ True],\n",
      "       [ True],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool), 1.0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt(\"./data/train_xor.txt\")\n",
    "x_data = xy[:, 0:-1] # 4x2\n",
    "y_data = xy[:, -1].reshape(4, 1) # 4x1 ( o ) / 1x4 로 했더니 test 에서 문제발생\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 5], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([5, 4], -1.0, 1.0))\n",
    "W3 = tf.Variable(tf.random_uniform([4, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([5]), name=\"Bias1\") # node 5개 -> bias 도 5개\n",
    "b2 = tf.Variable(tf.zeros([4]), name=\"Bias2\") # node 4개 -> bias 도 4개\n",
    "b3 = tf.Variable(tf.zeros([1]), name=\"Bias2\") # node 1개 -> bias 도 1개\n",
    "\n",
    "# hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1) # layer 1의 출력 = layer 2의 입력\n",
    "L3 = tf.sigmoid(tf.matmul(L2, W2) + b2) # L2 를 입력으로 하는 sigmoid\n",
    "hypothesis = tf.sigmoid(tf.matmul(L3, W3) + b3) # L3 를 입력으로 하는 sigmoid\n",
    "\n",
    "# cost\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1.-hypothesis))\n",
    "\n",
    "# minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Fitting\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 5000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W1), sess.run(W2))\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y)\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
