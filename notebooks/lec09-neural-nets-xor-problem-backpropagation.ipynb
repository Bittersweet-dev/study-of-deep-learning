{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lecture 9. Neural Network 1: XOR 문제와 학습방법, Backpropagation (1986 breakthrough)\n",
    "- XOR 문제 딥러닝으로 풀기\n",
    "- 특별편: 10분안에 미분 정리하기\n",
    "- 딥넷트웍 학습 시키기 (backpropagation)\n",
    "- 실습1: XOR을 위한 텐스플로우 딥넷트웍 \n",
    "- 실습2: Tensor Board로 딥네트웍 들여다보기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One logistic regression unit cannot separate XOR\n",
    "- 하나의 모델로는 XOR 문제를 풀 수 없음\n",
    "\n",
    "### Multiple logistic regression units\n",
    "- 여러개의 모델로 풀 수 있음\n",
    "\n",
    "### Neural Network ( NN )\n",
    "- No one on earth had found a viable way to train\n",
    "- 그러나 수 많은 weights 를 학습시키기 어렵다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## XOR using NN\n",
    "- 예) 3개 Gate 로 XOR 문제 풀기\n",
    "\n",
    "![](./img/09-xor-problem-01.png)\n",
    "![](./img/09-xor-problem-02.png)\n",
    "![](./img/09-xor-problem-03.png)\n",
    "![](./img/09-xor-problem-04.png)\n",
    "\n",
    "### Forward propagation\n",
    "- 위 문제를 하나의 그림으로 표현 가능\n",
    "- Node : Gate, Unit, Perceptron 이라고 불림\n",
    "- Can you find another $w$ and $b$ for the XOR ?\n",
    "![](./img/09-xor-problem-05.png)\n",
    "\n",
    "- 각각의 $w$ vector 를 하나의 matrix 로 표현\n",
    "![](./img/09-xor-problem-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hypothesis\n",
    "- $K(X) = sigmoid({XW}_1 + b_1)$ --- 앞 단의 layer\n",
    "- $\\hat{y} = H(X) = sigmoid({K(X)W}_2 + b_2)$ --- 뒷 단의 layer\n",
    "\n",
    "```\n",
    "# NN\n",
    "K = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "hypothesis = tf.digmoid(tf.matmul(K, W2) + b2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## How can we learn $W_1, W_2, b_1, b_2$ from training data ?\n",
    "\n",
    "### Back propagation ( chain rule )\n",
    "- 뉴럴 네트워크에서 layer 가 많아질 수록 미분 과정은 굉장히 복잡함\n",
    " - 각각의 node 에서 weight의 미분을 구해야함 ( 미분 = 영향력, 기울기, 변화량 )\n",
    "- Backpropagation 1974, 1982 by Paul Werbos, 1986 by Hinton\n",
    " - 실제값과 예측값을 비교해서 나오는 error = cost 를 구하여\n",
    " - error 를 뒤에서 부터 앞으로 보내면서 미분값을 역으로 구하는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 예제를 통해 Backpropagation 이해하기\n",
    "\n",
    "### 1. forward \n",
    "- $w = -2, x = 5, b = 3$ 집어 넣어 계산\n",
    "\n",
    "### 2. backward\n",
    "- $w$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial w}$ \n",
    "- $x$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial x}$ \n",
    "- $b$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial b}$ \n",
    "\n",
    "![](./img/09-backpropagation-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 미분값의 의미\n",
    "- $g$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial g} = \\frac{\\partial }{\\partial g} (g + b) =  1$\n",
    "- $b$ 가 $f$ 에 미치는 영향 : $\\frac{\\partial f}{\\partial g} = \\frac{\\partial }{\\partial b} (g + b) =  1$\n",
    "- $w$ 가 $g$ 에 미치는 영향 : $\\frac{\\partial g}{\\partial w} = \\frac{\\partial }{\\partial w} (wx) =  x$\n",
    "- $x$ 가 $g$ 에 미치는 영향 : $\\frac{\\partial g}{\\partial x} = \\frac{\\partial }{\\partial x} (wx) =  w$\n",
    "\n",
    "![](./img/09-backpropagation-02.png)\n",
    "\n",
    "### Hand writing\n",
    "\n",
    "![](./img/09-backpropagation-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 미분값의 의미\n",
    "- $\\frac{\\partial f}{\\partial w} = 5$ 라는 것은 $w$ 가 변함에 따라 $f$ 는 5배 변함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 더 많은 layer 에서도 적용 가능\n",
    "- 아무리 많은 node 를 갖더라도 뒤에서 부터 chain rule 을 적용하면 모든 미분값을 구할 수 있음\n",
    "![](./img/09-backpropagation-03.png)\n",
    "\n",
    "### sigmoid 미분도 같은 원리로 진행 가능\n",
    "![](./img/09-backpropagation-04.png)\n",
    "\n",
    "### TensorBoard\n",
    "- TensorFlow 에서 Backporpagation 과정을 보여주는 그래프\n",
    "![](./img/09-backpropagation-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 9-1. NN for XOR & TensorBoard\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data set\n",
    "```\n",
    "# XOR\n",
    "# x1 x2 y\n",
    "0 0 0\n",
    "0 1 1\n",
    "1 0 1\n",
    "1 1 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### XOR with Logistic regression\n",
    "- Does not work !\n",
    "- accuracy is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "''' \n",
    "# same code\n",
    "xy = np.loadtxt('./data/train_xor.txt', unpack=True)\n",
    "x_data = xy[0:-1]\n",
    "y_data = xy[-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "# Hypothesis\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "'''\n",
    "\n",
    "xy = np.loadtxt('./data/train_xor.txt')\n",
    "x_data = xy[:,:-1]\n",
    "y_data = xy[:,-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "# Hypothesis\n",
    "h = tf.matmul(X, W)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "\n",
    "# Cost\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "# Minimize\n",
    "a = tf.Variable(0.01) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Fit the line\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W))\n",
    "            \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y) # floor 1의자리 수 반환\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NN for XOR\n",
    "![](./img/09-xor-problem-06.png)\n",
    "\n",
    "- input X is 2 and sample is 4, so matrix X is 4x2\n",
    "- layer 1 is 2 node(output is 2), so weight W1 is 2x2\n",
    "- layer 2 is 1 node(output is 1), so weight W2 is 2x1\n",
    "- output y is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xy = np.loadtxt(\"./data/train_xor.txt\")\n",
    "x_data = xy[:, 0:-1] # 4x2\n",
    "y_data = xy[:, -1].reshape(4, 1) # 4x1 ( o ) / 1x4 로 했더니 test 에서 문제발생\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]), name=\"Bias1\") # node 2개 -> bias 도 2개\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\") # node 1개 -> bias 도 1개\n",
    "\n",
    "# hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1) # layer 1의 출력 = layer 2의 입력\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2) # L2 를 입력으로 하는 sigmoid\n",
    "\n",
    "# cost\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1.-hypothesis))\n",
    "\n",
    "# minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Fitting\n",
    "    for step in range(5001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W1), sess.run(W2))\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Wide NN for XOR\n",
    "- layer 1 is 2 x 10\n",
    "- layer 2 is 10 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xy = np.loadtxt(\"./data/train_xor.txt\")\n",
    "x_data = xy[:, 0:-1] # 4x2\n",
    "y_data = xy[:, -1].reshape(4, 1) # 4x1 ( o ) / 1x4 로 했더니 test 에서 문제발생\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([10, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10]), name=\"Bias1\") # node 10개 -> bias 도 10개\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\") # node 1개 -> bias 도 1개\n",
    "\n",
    "# hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1) # layer 1의 출력 = layer 2의 입력\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2) # L2 를 입력으로 하는 sigmoid\n",
    "\n",
    "# cost\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1.-hypothesis))\n",
    "\n",
    "# minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Fitting\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 5000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W1), sess.run(W2))\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Deep NN for XOR\n",
    "- layer 1 is 2x5\n",
    "- layer 2 is 5x4\n",
    "- layer 3 is 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xy = np.loadtxt(\"./data/train_xor.txt\")\n",
    "x_data = xy[:, 0:-1] # 4x2\n",
    "y_data = xy[:, -1].reshape(4, 1) # 4x1 ( o ) / 1x4 로 했더니 test 에서 문제발생\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 5], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([5, 4], -1.0, 1.0))\n",
    "W3 = tf.Variable(tf.random_uniform([4, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([5]), name=\"Bias1\") # node 5개 -> bias 도 5개\n",
    "b2 = tf.Variable(tf.zeros([4]), name=\"Bias2\") # node 4개 -> bias 도 4개\n",
    "b3 = tf.Variable(tf.zeros([1]), name=\"Bias2\") # node 1개 -> bias 도 1개\n",
    "\n",
    "# hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1) # layer 1의 출력 = layer 2의 입력\n",
    "L3 = tf.sigmoid(tf.matmul(L2, W2) + b2) # L2 를 입력으로 하는 sigmoid\n",
    "hypothesis = tf.sigmoid(tf.matmul(L3, W3) + b3) # L3 를 입력으로 하는 sigmoid\n",
    "\n",
    "# cost\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1.-hypothesis))\n",
    "\n",
    "# minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Fitting\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 5000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, y:y_data}), sess.run(W1), sess.run(W2))\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 9-2. TensorBoard\n",
    "---\n",
    "- Visualize your TF graph\n",
    "- Plot quantitative metrics\n",
    "- Show additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](./img/09-tensorboard-01.png)\n",
    "![](./img/09-tensorboard-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5 Steps of using tensorboard\n",
    "![](./img/09-tensorboard-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. From Tf graph, decide which node you want to annotate\n",
    " - 그래프(노드) 이름을 정해줌\n",
    " - `with tf.name_scope(\"test\") as scope:`\n",
    " - 데이터는 scalar 값, histogram(vector 값) 을 가짐, 이를 변수들에 넣어줌\n",
    " - `th.histogram_summary(\"weights\", W), tf.scalar_summary(\"accuracy\", accuracy)`\n",
    "2. Merge all summaries\n",
    " - operate 하기 전에 위에서 만들어준 변수를 다 포함시켜 하나의 오퍼레이션으로 통합시킴\n",
    " - `merged = tf.merge_all_summaries()`\n",
    "3. Create writer\n",
    " - 어디에 쓸건지 정해줌, 디렉토리 이름과 세션의 그래프를 적어줌\n",
    " - `writer = tf.train.SummaryWriter(\"/tmp/mnist_logs\", sess.graph_def)`\n",
    "4. Run summary merge and add_summary\n",
    " - 실행시킴, 실행시킨 결과가 summary\n",
    " - `summary = sess.run(merged, ...); writer.add_summary(summary);`\n",
    "5. Launch Tensorboard\n",
    " - 커맨드라인에서 텐서보드를 실행\n",
    " - `tensorboard --logdir=/tmp/mnist_logs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Name variables\n",
    "- 변수들에 모두 이름을 넣어줌\n",
    "\n",
    "```\n",
    "X = tf.placeholder(tf.float32, name = \"X-input\")\n",
    "y = tf.placeholder(tf.float32, name = \"y-input\")\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0), name = \"Weight1\")\n",
    "W2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0), name = \"Weight2\")\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]), name = \"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name = \"Bias2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Add histogram\n",
    "- tf.histogram_summary()\n",
    "\n",
    "```\n",
    "w1_hist = tf.histogram_summary(\"weights1\", W1)\n",
    "w2_hist = tf.histogram_summary(\"weights2\", W2)\n",
    "\n",
    "b1_hist = tf.histogram_summary(\"biases1\", b1)\n",
    "b2_hist = tf.histogram_summary(\"biases2\", b2)\n",
    "\n",
    "y_hist = tf.histogram_summary(\"y\", y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Add scope for better graph hierarch\n",
    "- with 를 사용하여 layer2, layer3, cost, ... 에 관계된 것만 아래에 쓰면서 변수들을 그루핑함\n",
    "\n",
    "```\n",
    "# hypothesis\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    hypothesis = tf.sigmoid(tf.matmul(L2, W12) + b2)\n",
    "    \n",
    "# cost function\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "    cost_summ = tf.scalar_summary(\"cost\", cost)\n",
    "\n",
    "# minimize\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train = optimizer.minimize(cost)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Add scalar variables\n",
    "- tf.scalar_summary()\n",
    "\n",
    "```\n",
    "# cost function\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "    cost_summ = tf.scalar_summary(\"cost\", cost)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### merge summaries and create writer after creating session\n",
    "- 세션을 열고 지금까지 만들어 놓은 써머리를 모두 머지 ( merged ) \n",
    "- 파일을 열듯, 위치를 주고 그래프 정보를 주어 writer 에 저장, writer 를 출력하면 됨\n",
    "\n",
    "```\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(\"./logs/xor_logs\", sess.graph)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run merged summary and write ( add summary )\n",
    "- 1번 case )\n",
    " - 매번 실행을 시킬때마다 merged, train 을 실행 시킴\n",
    " - merged 의 결과를 summary 에 담아 출력\n",
    "\n",
    "- 2번 case )\n",
    " - 학습은 매번 하지만, 값의 출력은 2000번에 한번씩 출력\n",
    "\n",
    "```\n",
    "# Fit the line.\n",
    "for step in range(20000):\n",
    "    summary, _ = sess.run([merged, train], feed_dict={X:x_data, y:y_data})\n",
    "    writer.add_summary(summary, step)\n",
    "    \n",
    "# Fit the line.\n",
    "for step in range(20000):\n",
    "    sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "    if step % 2000 == 0:\n",
    "        summary, _ = sess.run([merged, train], feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Launch TensorBoard\n",
    "\n",
    "![](./img/09-tensorboard-04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# load data\n",
    "xy = np.loadtxt(\"./data/train_xor.txt\")\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, -1].reshape(4, 1)\n",
    "\n",
    "X = tf.placeholder(tf.float32, name = \"X-input\")\n",
    "y = tf.placeholder(tf.float32, name = \"y-input\")\n",
    "\n",
    "# Varialbe\n",
    "W1 = tf.Variable(tf.random_uniform([2, 5], -1.0, 1.0), name = \"weight1\")\n",
    "W2 = tf.Variable(tf.random_uniform([5, 4], -1.0, 1.0), name = \"wieght2\")\n",
    "W3 = tf.Variable(tf.random_uniform([4, 1], -1.0, 1.0), name = \"weight3\")\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([5]), name = \"bias1\")\n",
    "b2 = tf.Variable(tf.zeros([4]), name = \"bias2\")\n",
    "b3 = tf.Variable(tf.zeros([1]), name = \"bias3\")\n",
    "\n",
    "# Add histogram\n",
    "w1_hist = tf.histogram_summary(\"weight1\", W1)\n",
    "w2_hist = tf.histogram_summary(\"wieght2\", W2)\n",
    "w3_hist = tf.histogram_summary(\"weight3\", W3)\n",
    "\n",
    "b1_hist = tf.histogram_summary(\"bias1\", b1)\n",
    "b2_hist = tf.histogram_summary(\"bias2\", b2)\n",
    "b3_hist = tf.histogram_summary(\"bias3\", b3)\n",
    "\n",
    "y_hist = tf.histogram_summary(\"y\", y)\n",
    "\n",
    "\n",
    "# hypothesis\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    L3 = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "\n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    hypothesis = tf.sigmoid(tf.matmul(L3, W3) + b3)\n",
    "    \n",
    "# cost\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "    cost_summ = tf.scalar_summary(\"cost\", cost)\n",
    "    \n",
    "# minimize\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # tensorboard merge\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(\"./logs/xor_logs\", sess.graph)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # run graph\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            summary, _ = sess.run([merged, train], feed_dict={X:x_data, y:y_data})\n",
    "            writer.add_summary(summary, step)\n",
    "    \n",
    "    # test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # calculate accuracy\n",
    "    print(sess.run([hypothesis, \n",
    "                    tf.floor(hypothesis+0.5), correct_prediction, accuracy], \n",
    "                    feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- 몇가지 변경된 Warnning 코드들 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.0048157 ],\n",
      "       [ 0.98759717],\n",
      "       [ 0.98962003],\n",
      "       [ 0.01553397]], dtype=float32), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[ True],\n",
      "       [ True],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool), 1.0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# load data\n",
    "xy = np.loadtxt(\"./data/train_xor.txt\")\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, -1].reshape(4, 1)\n",
    "\n",
    "X = tf.placeholder(tf.float32, name = \"X-input\")\n",
    "y = tf.placeholder(tf.float32, name = \"y-input\")\n",
    "\n",
    "# Varialbe\n",
    "W1 = tf.Variable(tf.random_uniform([2, 5], -1.0, 1.0), name = \"weight1\")\n",
    "W2 = tf.Variable(tf.random_uniform([5, 4], -1.0, 1.0), name = \"wieght2\")\n",
    "W3 = tf.Variable(tf.random_uniform([4, 1], -1.0, 1.0), name = \"weight3\")\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([5]), name = \"bias1\")\n",
    "b2 = tf.Variable(tf.zeros([4]), name = \"bias2\")\n",
    "b3 = tf.Variable(tf.zeros([1]), name = \"bias3\")\n",
    "\n",
    "# Add histogram\n",
    "w1_hist = tf.summary.histogram(\"weight1\", W1)\n",
    "w2_hist = tf.summary.histogram(\"wieght2\", W2)\n",
    "w3_hist = tf.summary.histogram(\"weight3\", W3)\n",
    "\n",
    "b1_hist = tf.summary.histogram(\"bias1\", b1)\n",
    "b2_hist = tf.summary.histogram(\"bias2\", b2)\n",
    "b3_hist = tf.summary.histogram(\"bias3\", b3)\n",
    "\n",
    "y_hist = tf.summary.histogram(\"y\", y)\n",
    "\n",
    "\n",
    "# hypothesis\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    L3 = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "\n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    hypothesis = tf.sigmoid(tf.matmul(L3, W3) + b3)\n",
    "    \n",
    "# cost\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = -tf.reduce_mean(y*tf.log(hypothesis) + (1-y)*tf.log(1-hypothesis))\n",
    "    cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "# minimize\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # tensorboard merge\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs\", sess.graph)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # run graph\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            summary, _ = sess.run([merged, train], feed_dict={X:x_data, y:y_data})\n",
    "            writer.add_summary(summary, step)\n",
    "    \n",
    "    # test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # calculate accuracy\n",
    "    print(sess.run([hypothesis, \n",
    "                    tf.floor(hypothesis+0.5), correct_prediction, accuracy], \n",
    "                    feed_dict={X:x_data, y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Launch tensorbard in terminal\n",
    "- `$ tensorboard --logdir=./logs/xor_logs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tensorboard 실행시 주의점\n",
    "- Jupyter Notebook 에서 실행후 위의 명령을 터미널에서 타이핑\n",
    " - xor_logs 폴더를 의미함 ( 폴더 내부에 log 파일이 쌓임, 매 실행마다, 그리고 커맨드라인에서는 Warning 메세지 뜨나 상관없음 )\n",
    "- 재실행하고 싶으면 Jupyter Notebook 을 restart 하고 다시 실행하여야 함\n",
    " - 만약 바로 재실행시 아웃풋 오류메세지 뜨고, Tensorboard 에서는 중복 실행되어 그래프 나타남"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
